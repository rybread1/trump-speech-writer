{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trump_speech_writer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhCPlRhLXXUyV7sHMDq2Aj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rybread1/trump_speech_writer/blob/master/trump_speech_writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqId2KNgX1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import os"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vp-4AMHHWGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "90ce8ce1-26b2-4a9a-ef29-97d40fe07fa5"
      },
      "source": [
        "!curl -O https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  903k  100  903k    0     0  3135k      0 --:--:-- --:--:-- --:--:-- 3135k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McvkTbG5F69P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reading and processing text\n",
        "with open('speeches.txt', 'r') as fp:\n",
        "    text = fp.read()\n",
        "    \n",
        "start_indx = text.find('Thank you so much')\n",
        "\n",
        "text = text[start_indx:].lower()  # trimmed text doc\n",
        "char_set = set(text) # unique character set\n",
        "char_set_sorted = sorted(char_set)\n",
        "\n",
        "char_2_int_dict = {ch:i for i,ch in enumerate(char_set_sorted)} # dict mapping char to int\n",
        "char_array = np.array(char_set_sorted) # array mapping idx to char\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char_2_int_dict[ch] for ch in text],\n",
        "    dtype=np.int32)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mjYtzdJIck-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
        "\n",
        "seq_length = 100 \n",
        "ds_chunks = ds_text_encoded.batch(seq_length+1, drop_remainder=True) \n",
        "\n",
        "## define the function for splitting x & y\n",
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1]\n",
        "    target_seq = chunk[1:]\n",
        "    return input_seq, target_seq\n",
        "\n",
        "ds_sequences = ds_chunks.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 200000\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "def get_test_train_split(text, seq_length, batch_size, train_split=0.95):\n",
        "    return np.floor(len(text) / seq_length / batch_size) * train_split\n",
        "\n",
        "train_batches = get_test_train_split(text_encoded, seq_length, BATCH_SIZE)\n",
        "\n",
        "ds_train = ds.take(train_batches)\n",
        "ds_valid = ds.skip(train_batches)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajZHQBa8SLsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35664c32-e2e7-4406-b361-1cc54eb65e90"
      },
      "source": [
        "def build_model(input_size, vocab_size, embedding_dim, rnn_units, dropout=True):\n",
        "    inputs = tf.keras.Input(input_size)\n",
        "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "    x = tf.keras.layers.LSTM(rnn_units, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.LSTM(rnn_units, return_sequences=True)(x)\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "model = build_model(input_size=seq_length, vocab_size=len(char_array), \n",
        "                    embedding_dim=256, rnn_units=1024)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "model.summary()\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=0)\n",
        "\n",
        "results = model.fit(ds_train, validation_data=ds_valid, epochs=100, callbacks=[cp_callback])\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 100, 256)          16896     \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 100, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 100, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100, 66)           67650     \n",
            "=================================================================\n",
            "Total params: 13,724,226\n",
            "Trainable params: 13,724,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "133/133 [==============================] - 27s 201ms/step - loss: 2.8563 - val_loss: 2.2488\n",
            "Epoch 2/100\n",
            "133/133 [==============================] - 26s 198ms/step - loss: 1.8924 - val_loss: 1.5760\n",
            "Epoch 3/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 1.4429 - val_loss: 1.2889\n",
            "Epoch 4/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 1.2466 - val_loss: 1.1852\n",
            "Epoch 5/100\n",
            "133/133 [==============================] - 26s 196ms/step - loss: 1.1379 - val_loss: 1.0645\n",
            "Epoch 6/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 1.0601 - val_loss: 1.0014\n",
            "Epoch 7/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.9960 - val_loss: 0.9266\n",
            "Epoch 8/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.9350 - val_loss: 0.8557\n",
            "Epoch 9/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.8754 - val_loss: 0.8058\n",
            "Epoch 10/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.8119 - val_loss: 0.7270\n",
            "Epoch 11/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.7462 - val_loss: 0.6677\n",
            "Epoch 12/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.6769 - val_loss: 0.5826\n",
            "Epoch 13/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.6099 - val_loss: 0.5109\n",
            "Epoch 14/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.5395 - val_loss: 0.4521\n",
            "Epoch 15/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.4732 - val_loss: 0.3883\n",
            "Epoch 16/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.4111 - val_loss: 0.3313\n",
            "Epoch 17/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.3529 - val_loss: 0.2774\n",
            "Epoch 18/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.3041 - val_loss: 0.2434\n",
            "Epoch 19/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.2579 - val_loss: 0.1993\n",
            "Epoch 20/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.2220 - val_loss: 0.1749\n",
            "Epoch 21/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1925 - val_loss: 0.1555\n",
            "Epoch 22/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.1718 - val_loss: 0.1430\n",
            "Epoch 23/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1555 - val_loss: 0.1335\n",
            "Epoch 24/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1443 - val_loss: 0.1210\n",
            "Epoch 25/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1351 - val_loss: 0.1174\n",
            "Epoch 26/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.1300 - val_loss: 0.1103\n",
            "Epoch 27/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.1268 - val_loss: 0.1079\n",
            "Epoch 28/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.1261 - val_loss: 0.1104\n",
            "Epoch 29/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1278 - val_loss: 0.1162\n",
            "Epoch 30/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.1333 - val_loss: 0.1208\n",
            "Epoch 31/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.1411 - val_loss: 0.1265\n",
            "Epoch 32/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.1507 - val_loss: 0.1321\n",
            "Epoch 33/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1554 - val_loss: 0.1358\n",
            "Epoch 34/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.1498 - val_loss: 0.1265\n",
            "Epoch 35/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.1372 - val_loss: 0.1107\n",
            "Epoch 36/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1220 - val_loss: 0.1010\n",
            "Epoch 37/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1112 - val_loss: 0.0928\n",
            "Epoch 38/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1026 - val_loss: 0.0873\n",
            "Epoch 39/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0960 - val_loss: 0.0808\n",
            "Epoch 40/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0915 - val_loss: 0.0806\n",
            "Epoch 41/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0887 - val_loss: 0.0774\n",
            "Epoch 42/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0866 - val_loss: 0.0745\n",
            "Epoch 43/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0848 - val_loss: 0.0756\n",
            "Epoch 44/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0836 - val_loss: 0.0723\n",
            "Epoch 45/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0828 - val_loss: 0.0733\n",
            "Epoch 46/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0820 - val_loss: 0.0732\n",
            "Epoch 47/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0816 - val_loss: 0.0732\n",
            "Epoch 48/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0815 - val_loss: 0.0722\n",
            "Epoch 49/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0816 - val_loss: 0.0749\n",
            "Epoch 50/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0815 - val_loss: 0.0744\n",
            "Epoch 51/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0830 - val_loss: 0.0772\n",
            "Epoch 52/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.1698 - val_loss: 0.4976\n",
            "Epoch 53/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.6546 - val_loss: 0.4348\n",
            "Epoch 54/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.3990 - val_loss: 0.2507\n",
            "Epoch 55/100\n",
            "133/133 [==============================] - 26s 196ms/step - loss: 0.2499 - val_loss: 0.1693\n",
            "Epoch 56/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1699 - val_loss: 0.1207\n",
            "Epoch 57/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1250 - val_loss: 0.0950\n",
            "Epoch 58/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1011 - val_loss: 0.0837\n",
            "Epoch 59/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0886 - val_loss: 0.0745\n",
            "Epoch 60/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0822 - val_loss: 0.0717\n",
            "Epoch 61/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0790 - val_loss: 0.0718\n",
            "Epoch 62/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0774 - val_loss: 0.0713\n",
            "Epoch 63/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0765 - val_loss: 0.0693\n",
            "Epoch 64/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0760 - val_loss: 0.0695\n",
            "Epoch 65/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0759 - val_loss: 0.0705\n",
            "Epoch 66/100\n",
            "133/133 [==============================] - 26s 196ms/step - loss: 0.0757 - val_loss: 0.0658\n",
            "Epoch 67/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0755 - val_loss: 0.0685\n",
            "Epoch 68/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0756 - val_loss: 0.0690\n",
            "Epoch 69/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0756 - val_loss: 0.0676\n",
            "Epoch 70/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0695\n",
            "Epoch 71/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.0755 - val_loss: 0.0689\n",
            "Epoch 72/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0757 - val_loss: 0.0699\n",
            "Epoch 73/100\n",
            "133/133 [==============================] - 26s 196ms/step - loss: 0.0756 - val_loss: 0.0682\n",
            "Epoch 74/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0678\n",
            "Epoch 75/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0757 - val_loss: 0.0671\n",
            "Epoch 76/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0759 - val_loss: 0.0697\n",
            "Epoch 77/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0777 - val_loss: 0.0744\n",
            "Epoch 78/100\n",
            "133/133 [==============================] - 26s 196ms/step - loss: 0.4258 - val_loss: 0.6833\n",
            "Epoch 79/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.6467 - val_loss: 0.4251\n",
            "Epoch 80/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.4226 - val_loss: 0.2827\n",
            "Epoch 81/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.2912 - val_loss: 0.2022\n",
            "Epoch 82/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.2086 - val_loss: 0.1473\n",
            "Epoch 83/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.1561 - val_loss: 0.1132\n",
            "Epoch 84/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.1217 - val_loss: 0.0954\n",
            "Epoch 85/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.1008 - val_loss: 0.0820\n",
            "Epoch 86/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0883 - val_loss: 0.0751\n",
            "Epoch 87/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0812 - val_loss: 0.0714\n",
            "Epoch 88/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0775 - val_loss: 0.0694\n",
            "Epoch 89/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0678\n",
            "Epoch 90/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0747 - val_loss: 0.0691\n",
            "Epoch 91/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0741 - val_loss: 0.0665\n",
            "Epoch 92/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0739 - val_loss: 0.0668\n",
            "Epoch 93/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0738 - val_loss: 0.0671\n",
            "Epoch 94/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0736 - val_loss: 0.0663\n",
            "Epoch 95/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0737 - val_loss: 0.0663\n",
            "Epoch 96/100\n",
            "133/133 [==============================] - 26s 194ms/step - loss: 0.0737 - val_loss: 0.0674\n",
            "Epoch 97/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0736 - val_loss: 0.0670\n",
            "Epoch 98/100\n",
            "133/133 [==============================] - 26s 195ms/step - loss: 0.0737 - val_loss: 0.0666\n",
            "Epoch 99/100\n",
            "133/133 [==============================] - 26s 193ms/step - loss: 0.0737 - val_loss: 0.0680\n",
            "Epoch 100/100\n",
            "133/133 [==============================] - 26s 192ms/step - loss: 0.0737 - val_loss: 0.0678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x58f2dPbJC3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# @tf.function\n",
        "# def train_step(inp, target):\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     predictions = model(inp)\n",
        "#     loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
        "#   grads = tape.gradient(loss, model.trainable_variables)\n",
        "#   optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "#   return loss\n",
        "\n",
        "\n",
        "# # Training step\n",
        "# EPOCHS = 50\n",
        "# checkpoint_dir = 'training_1'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "  \n",
        "#   # resetting the hidden state at the start of every epoch\n",
        "#   model.reset_states()\n",
        "\n",
        "#   for (batch_n, (inp, target)) in enumerate(ds):\n",
        "#     loss = train_step(inp, target)\n",
        "\n",
        "#     if batch_n % 100 == 0:\n",
        "#       template = 'Epoch {} Batch {} Loss {}'\n",
        "#       print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "#   # saving (checkpoint) the model every 5 epochs\n",
        "#   if (epoch + 1) % 5 == 0:\n",
        "#     model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "#   print('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "\n",
        "# model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewli0cteLEt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, starting_str, \n",
        "           len_generated_text=500, \n",
        "           max_input_length=80,\n",
        "           scale_factor=1.0):\n",
        "    \n",
        "    starting_str = starting_str.lower()\n",
        "    encoded_input = [char_2_int_dict[s] for s in starting_str]\n",
        "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(len_generated_text):\n",
        "        logits = model(encoded_input)        \n",
        "        logits = tf.squeeze(logits, 0)\n",
        "\n",
        "        scaled_logits = logits * scale_factor\n",
        "        new_char_indx = tf.random.categorical(scaled_logits, num_samples=1)\n",
        "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()          \n",
        "        generated_str += str(char_array[new_char_indx])\n",
        "        \n",
        "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
        "\n",
        "        encoded_input = tf.concat(\n",
        "            [encoded_input, new_char_indx],\n",
        "            axis=1)\n",
        "        encoded_input = encoded_input[:, -max_input_length:]\n",
        "\n",
        "    return generated_str\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmEF68q6M8YH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "9510c8ad-cdcf-406d-f961-42b69f358844"
      },
      "source": [
        "generated_text = generate_text(model, \n",
        "                               starting_str='we will make america great again', \n",
        "                               scale_factor=2, \n",
        "                               len_generated_text=5000,\n",
        "                               max_input_length=seq_length)\n",
        "\n",
        "print(generated_text)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we will make america great again. we will make america great again. thank you. thank you.\n",
            "\n",
            "\n",
            "...well, look he’s [referring to cruz] from texas. to the best of my knowledge, the united states parade that they used to smill and they're going to sell cars, trucks and parts into the united states. we need that thinking. we have the opposital that we have to do in terms of isis and all. but we have to start rebuilding our country.\n",
            "we have $4 billion in mexico. you’re born a baby in mexico, it’s like bye-bye.\n",
            "here, you’re born over here, boed, you believe it?\n",
            "here we have generals. they go on television and talk. they do all talk, they’re going to call it the trump wall. i said, well have to do what they're doing.  they are devaluing down to nothing.  and what it's going to do is make america great again? i promise. thank you very much. i really appreciate it and everybody appreciates it.\n",
            "because there will pay for the wall and it's very easy. they don’t use — you know what? i’ll tell you something. in the end, the people that gives these millions and millions of dollars on doing something that you can do for $2,000.\n",
            "you know, when you look at what’s happening with corporations and the highest tax – you know what, we do not go about it. they can’t get their money back and because they get locks the job done. america should have all over the place. $2 trillion. we have nothing. we got nothing.\n",
            "so we get bergdahl, and that’s what we get. and they get five people that are right now back out of the labor force. 93 million people. of the 93 many of the money went out quite a while ago. some of it went out more recently. but all of this has gone burget to say the words radical islam. here is what she said: \"muslims are peaceful and tolerant people, and have nothing whatsoever to do with terrorism.\"\n",
            "\n",
            "\n",
            "hillary clinton says the solution is one of the different shows. i didn’t want to go into iraq. all these guys think that i’m a tough guy, that negotiators in the world, and i put them one for each country. believe me, folks. we will do very, very well reserves. we went to do a book at the world. we can’t let that happen.\n",
            "we’ve lost our manufacturing. millions and millions of jobs, thousands and thousands of good things would happen. and some of the more fair reporters with the deal doesn’t even see a group of deals with china. i made a fortune. okay? they go to washington – \"we’re going to stop obamacare. we’re going to be so smart. we’ve got to be smart. we’ve got to be a country again.\n",
            "so i just want to thank – this endorsement meant so much to me from new hampshire. she’s horrible. she’s horrible.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " and you know really don’t — i’ll tell you who does not like it down. you’ve got to get it done. that means i’m going to get it done.\n",
            "but we are going to win against a lot in jather seventor. but we’re going to win it. and a simple act of congress gets rid of it. and a lot of people don’t know we protect japan. do you know that? does anybody wants to do with them. so i don’t know if it was presidential, honestly, whether represent want them is absolutely incredible that we handed them over $150 billion to do everybody. we have to get so, so, so vigilant, and they say \"the young married couple\". that’s not going to happen with me.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "…well, jeb bush is a very nice man. i mean this so, so strongly – the conor documentators are right now worse than 12 years ago and that’s a shame\n",
            "\n",
            "\n",
            "…tell you this: of course if you look at what’s going on and you look at the tremendous numbers of jobs.\n",
            "only the best in the world. we’re sending fighters every single thing we can do on the way they’re all doing it for that reason.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "i think it’s irrelevant. i just – you know, i have a good chance of winning because you look at what’s going on at the airports and you look at that. on the other side of the mountain china is taking out all the minutes. they don’t love me so much anymore. but i don’t care.\n",
            "and we’re getting rid of deductions that are actually obselete that really donald trump in deal. i could turn this all around. we’re going to do it by putting america first.\n",
            "that commitment is the foundation for change that’s been missing and it’s been missing for a long time.\n",
            "\n",
            "\n",
            "it’s important to me. you know, i’m from here. and he said, \"with wisconsin, you have a great policy plan and i’ve gotten a lot of credit for it and it’s on the cover of every newspaper. it’s something that hasn't happened — almost has not happened. chris got a poll and in competitor – they said \"he’ll never run but it’s a nice guy. i mean, this is one thing we all have learned – we can’t fix the rigged system by relying on very, and i mean such a great guy – i guess a lot of you folks saw that – but i’m watching...what we’re doing a lot of things to discuss, but we had some very bad news a few days ago, and you saw what happened. and then we have to read probably liby. so here’s what happened. it’s been a little controversial. i don’t want to subsidize these various countries where they come fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLDya2hDRNXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}