{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rybread1/trump_speech_writer/blob/master/trump_speech_writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYqId2KNgX1j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "0vp-4AMHHWGA",
    "outputId": "90ce8ce1-26b2-4a9a-ef29-97d40fe07fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  903k  100  903k    0     0  1204k      0 --:--:-- --:--:-- --:--:-- 1204k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McvkTbG5F69P"
   },
   "outputs": [],
   "source": [
    "## Reading and processing text\n",
    "with open('speeches.txt', 'r') as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "start_indx = text.find('Thank you so much')\n",
    "\n",
    "text = text[start_indx:].lower()  # trimmed text doc\n",
    "char_set = set(text) # unique character set\n",
    "char_set_sorted = sorted(char_set)\n",
    "\n",
    "char_2_int_dict = {ch:i for i,ch in enumerate(char_set_sorted)} # dict mapping char to int\n",
    "char_array = np.array(char_set_sorted) # array mapping idx to char\n",
    "\n",
    "text_encoded = np.array(\n",
    "    [char_2_int_dict[ch] for ch in text],\n",
    "    dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mjYtzdJIck-"
   },
   "outputs": [],
   "source": [
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "seq_length = 100 \n",
    "ds_chunks = ds_text_encoded.batch(seq_length+1, drop_remainder=True) \n",
    "\n",
    "## define the function for splitting x & y\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "def get_test_train_split(text, seq_length, batch_size, train_split=0.95):\n",
    "    return np.floor(len(text) / seq_length / batch_size) * train_split\n",
    "\n",
    "train_batches = get_test_train_split(text_encoded, seq_length, BATCH_SIZE)\n",
    "\n",
    "ds_train = ds.take(train_batches)\n",
    "ds_valid = ds.skip(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ajZHQBa8SLsv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "35664c32-e2e7-4406-b361-1cc54eb65e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 100, 256)          16896     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100, 1024)         5246976   \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 100, 1024)         8392704   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100, 66)           67650     \n",
      "=================================================================\n",
      "Total params: 13,724,226\n",
      "Trainable params: 13,724,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 27s 201ms/step - loss: 2.8563 - val_loss: 2.2488\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 26s 198ms/step - loss: 1.8924 - val_loss: 1.5760\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 1.4429 - val_loss: 1.2889\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 1.2466 - val_loss: 1.1852\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 26s 196ms/step - loss: 1.1379 - val_loss: 1.0645\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 1.0601 - val_loss: 1.0014\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.9960 - val_loss: 0.9266\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.9350 - val_loss: 0.8557\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.8754 - val_loss: 0.8058\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.8119 - val_loss: 0.7270\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.7462 - val_loss: 0.6677\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.6769 - val_loss: 0.5826\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.6099 - val_loss: 0.5109\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.5395 - val_loss: 0.4521\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.4732 - val_loss: 0.3883\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.4111 - val_loss: 0.3313\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.3529 - val_loss: 0.2774\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.3041 - val_loss: 0.2434\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.2579 - val_loss: 0.1993\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.2220 - val_loss: 0.1749\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1925 - val_loss: 0.1555\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.1718 - val_loss: 0.1430\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1555 - val_loss: 0.1335\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1443 - val_loss: 0.1210\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1351 - val_loss: 0.1174\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.1300 - val_loss: 0.1103\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.1268 - val_loss: 0.1079\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.1261 - val_loss: 0.1104\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1278 - val_loss: 0.1162\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.1333 - val_loss: 0.1208\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.1411 - val_loss: 0.1265\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.1507 - val_loss: 0.1321\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1554 - val_loss: 0.1358\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.1498 - val_loss: 0.1265\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.1372 - val_loss: 0.1107\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1220 - val_loss: 0.1010\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1112 - val_loss: 0.0928\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1026 - val_loss: 0.0873\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0960 - val_loss: 0.0808\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0915 - val_loss: 0.0806\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0887 - val_loss: 0.0774\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0866 - val_loss: 0.0745\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0848 - val_loss: 0.0756\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0836 - val_loss: 0.0723\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0828 - val_loss: 0.0733\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0820 - val_loss: 0.0732\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0816 - val_loss: 0.0732\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0815 - val_loss: 0.0722\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0816 - val_loss: 0.0749\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0815 - val_loss: 0.0744\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0830 - val_loss: 0.0772\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.1698 - val_loss: 0.4976\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.6546 - val_loss: 0.4348\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.3990 - val_loss: 0.2507\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 26s 196ms/step - loss: 0.2499 - val_loss: 0.1693\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1699 - val_loss: 0.1207\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1250 - val_loss: 0.0950\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1011 - val_loss: 0.0837\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0886 - val_loss: 0.0745\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0822 - val_loss: 0.0717\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0790 - val_loss: 0.0718\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0774 - val_loss: 0.0713\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0765 - val_loss: 0.0693\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0760 - val_loss: 0.0695\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0759 - val_loss: 0.0705\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 26s 196ms/step - loss: 0.0757 - val_loss: 0.0658\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0755 - val_loss: 0.0685\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0756 - val_loss: 0.0690\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0756 - val_loss: 0.0676\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0695\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.0755 - val_loss: 0.0689\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0757 - val_loss: 0.0699\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 26s 196ms/step - loss: 0.0756 - val_loss: 0.0682\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0678\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0757 - val_loss: 0.0671\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0759 - val_loss: 0.0697\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0777 - val_loss: 0.0744\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 26s 196ms/step - loss: 0.4258 - val_loss: 0.6833\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.6467 - val_loss: 0.4251\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.4226 - val_loss: 0.2827\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.2912 - val_loss: 0.2022\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.2086 - val_loss: 0.1473\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.1561 - val_loss: 0.1132\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.1217 - val_loss: 0.0954\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.1008 - val_loss: 0.0820\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0883 - val_loss: 0.0751\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0812 - val_loss: 0.0714\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0775 - val_loss: 0.0694\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0756 - val_loss: 0.0678\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0747 - val_loss: 0.0691\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0741 - val_loss: 0.0665\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0739 - val_loss: 0.0668\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0738 - val_loss: 0.0671\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0736 - val_loss: 0.0663\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0737 - val_loss: 0.0663\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 26s 194ms/step - loss: 0.0737 - val_loss: 0.0674\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0736 - val_loss: 0.0670\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 26s 195ms/step - loss: 0.0737 - val_loss: 0.0666\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 26s 193ms/step - loss: 0.0737 - val_loss: 0.0680\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 0.0737 - val_loss: 0.0678\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_size, vocab_size, embedding_dim, rnn_units, dropout=True):\n",
    "    inputs = tf.keras.Input(input_size)\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = tf.keras.layers.LSTM(rnn_units, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(rnn_units, return_sequences=True)(x)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = build_model(input_size=seq_length, vocab_size=len(char_array), \n",
    "                    embedding_dim=256, rnn_units=1024)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "model.summary()\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "results = model.fit(ds_train, validation_data=ds_valid, epochs=100, callbacks=[cp_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x58f2dPbJC3H"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(inp, target):\n",
    "#   with tf.GradientTape() as tape:\n",
    "#     predictions = model(inp)\n",
    "#     loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
    "#   grads = tape.gradient(loss, model.trainable_variables)\n",
    "#   optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#   return loss\n",
    "\n",
    "\n",
    "# # Training step\n",
    "# EPOCHS = 50\n",
    "# checkpoint_dir = 'training_1'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "  \n",
    "#   # resetting the hidden state at the start of every epoch\n",
    "#   model.reset_states()\n",
    "\n",
    "#   for (batch_n, (inp, target)) in enumerate(ds):\n",
    "#     loss = train_step(inp, target)\n",
    "\n",
    "#     if batch_n % 100 == 0:\n",
    "#       template = 'Epoch {} Batch {} Loss {}'\n",
    "#       print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "#   # saving (checkpoint) the model every 5 epochs\n",
    "#   if (epoch + 1) % 5 == 0:\n",
    "#     model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "#   print('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "\n",
    "# model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sample_weight_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-40256ebb0b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/ryan.osgar/Documents/repos/saved_models/trump_speech_writer_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/u01/local/anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   raise IOError(\n",
      "\u001b[0;32m/u01/local/anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       model.compile(**saving_utils.compile_args_from_training_config(\n\u001b[0;32m--> 126\u001b[0;31m           training_config))\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       logging.warning('No training configuration found in save file, so the '\n",
      "\u001b[0;32m/u01/local/anaconda/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    228\u001b[0m                                                     weighted_metrics_config)\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0msample_weight_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_weight_mode'"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/Users/ryan.osgar/Documents/repos/saved_models/trump_speech_writer_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ewli0cteLEt6"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           max_input_length=80,\n",
    "           scale_factor=1.0):\n",
    "    \n",
    "    starting_str = starting_str.lower()\n",
    "    encoded_input = [char_2_int_dict[s] for s in starting_str]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input)        \n",
    "        logits = tf.squeeze(logits, 0)\n",
    "\n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_indx = tf.random.categorical(scaled_logits, num_samples=1)\n",
    "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()          \n",
    "        generated_str += str(char_array[new_char_indx])\n",
    "        \n",
    "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
    "\n",
    "        encoded_input = tf.concat(\n",
    "            [encoded_input, new_char_indx],\n",
    "            axis=1)\n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "\n",
    "    return generated_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "gmEF68q6M8YH",
    "outputId": "9510c8ad-cdcf-406d-f961-42b69f358844"
   },
   "outputs": [],
   "source": [
    "generated_text = generate_text(model, \n",
    "                               starting_str='we will make america great again', \n",
    "                               scale_factor=2, \n",
    "                               len_generated_text=5000,\n",
    "                               max_input_length=seq_length)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLDya2hDRNXn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPhCPlRhLXXUyV7sHMDq2Aj",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "trump_speech_writer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
